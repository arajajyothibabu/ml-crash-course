{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation: Feature Engineering\n",
    "\n",
    "In traditional programming, the focus is on code. In machine learning projects, the focus shifts to representation. That is, one way developers hone a model is by adding and improving its features.\n",
    "\n",
    "#### Mapping Raw Data to Features\n",
    "\n",
    "Many machine learning models must represent the features as real-numbered vectors since the feature values must be multiplied by the model weights.\n",
    "\n",
    "#### Mapping numeric values\n",
    "\n",
    "Integer and floating-point data don't need a special encoding because they can be multiplied by a numeric weight. *converting the raw integer value 6 to the feature value 6.0 is trivial*\n",
    "\n",
    "#### Mapping categorical values\n",
    "Categorical features have a discrete set of possible values. For example, there might be a feature called `street_name` with options that include:\n",
    "\n",
    "`{'Charleston Road', 'North Shoreline Boulevard', 'Shorebird Way', 'Rengstorff Avenue'}`\n",
    "\n",
    "Since models cannot multiply strings by the learned weights, we use feature engineering to convert strings to numeric values.\n",
    "\n",
    "We can accomplish this by defining a mapping from the feature values, which we'll refer to as the vocabulary of possible values, to integers. Since not every street in the world will appear in our dataset, we can group all other streets into a catch-all \"other\" category, known as an **OOV (out-of-vocabulary)** bucket.\n",
    "\n",
    "Using this approach, here's how we can map our street names to numbers:\n",
    "\n",
    "- map Charleston Road to 0\n",
    "- map North Shoreline Boulevard to 1\n",
    "- map Shorebird Way to 2\n",
    "- map Rengstorff Avenue to 3\n",
    "- map everything else (OOV) to 4\n",
    "\n",
    "However, if we incorporate these index numbers directly into our model, it will impose some constraints that might be problematic:\n",
    "\n",
    "We'll be learning a single weight that applies to all streets. For example, if we learn a weight of 6 for street_name, then we will multiply it by 0 for Charleston Road, by 1 for North Shoreline Boulevard, 2 for Shorebird Way and so on. Consider a model that predicts house prices using `street_name` as a feature. It is unlikely that there is a linear adjustment of price based on the street name, and furthermore this would assume you have ordered the streets based on their average house price. Our model needs the flexibility of learning different weights for each street that will be added to the price estimated using the other features.\n",
    "\n",
    "We aren't accounting for cases where `street_name` may take multiple values. For example, many houses are located at the corner of two streets, and there's no way to encode that information in the `street_name` value if it contains a single index.\n",
    "\n",
    "To remove both these constraints, we can instead create a binary vector for each categorical feature in our model that represents values as follows:\n",
    "\n",
    "- For values that apply to the example, set corresponding vector elements to 1.\n",
    "- Set all other elements to 0.\n",
    "\n",
    "The length of this vector is equal to the number of elements in the vocabulary. This representation is called a one-hot encoding when a single value is 1, and a multi-hot encoding when multiple values are 1.\n",
    "\n",
    "#### Sparse Representation\n",
    "\n",
    "Suppose that you had 1,000,000 different street names in your data set that you wanted to include as values for street_name. Explicitly creating a binary vector of 1,000,000 elements where only 1 or 2 elements are true is a very inefficient representation in terms of both storage and computation time when processing these vectors. In this situation, a common approach is to use a [sparse representation](https://developers.google.com/machine-learning/glossary/#sparse_representation) in which only nonzero values are stored. In sparse representations, an independent model weight is still learned for each feature value, as described above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualities of Good Features\n",
    "\n",
    "- **Avoid rarely used discrete feature values**\n",
    "- **Prefer clear and obvious meanings**\n",
    "- **Don't mix \"magic\" values with actual data**\n",
    "    - quality_rating: 0.82\n",
    "    - quality_rating: 0.37\n",
    "    - *quality_rating: -1*\n",
    "    \n",
    "    To work around magic values, convert the feature into two features:\n",
    "\n",
    "        - One feature holds only quality ratings, never magic values.\n",
    "        - One feature holds a boolean value indicating whether or not a quality_rating was supplied. Give this boolean feature a name like is_quality_rating_defined\n",
    "- **Account for upstream instability**\n",
    "    - city_id: \"br/sao_paulo\"\n",
    "    - *inferred_city_cluster: \"219\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "#### Scaling feature values\n",
    "Scaling means converting floating-point feature values from their natural range (for example, 100 to 900) into a standard range (for example, 0 to 1 or -1 to +1). If a feature set consists of only a single feature, then scaling provides little to no practical benefit. If, however, a feature set consists of multiple features, then feature scaling provides the following benefits:\n",
    "\n",
    "- Helps gradient descent converge more quickly.\n",
    "- Helps avoid the \"NaN trap,\" in which one number in the model becomes a NaN (e.g., when a value exceeds the floating-point precision limit during training), and—due to math operations—every other number in the model also eventually becomes a NaN.\n",
    "- Helps the model learn appropriate weights for each feature. Without feature scaling, the model will pay too much attention to the features having a wider range.\n",
    "\n",
    "You don't have to give every floating-point feature exactly the same scale. Nothing terrible will happen if Feature A is scaled from -1 to +1 while Feature B is scaled from -3 to +3. However, your model will react poorly if Feature B is scaled from 5000 to 100000.\n",
    "\n",
    "> One obvious way to scale numerical data is to linearly map [min value, max value] to a small scale, such as [-1, +1].\n",
    "\n",
    "> Another popular scaling tactic is to calculate the Z score of each value. The Z score relates the number of standard deviations away from the mean.\n",
    "\n",
    "#### Handling extreme outliers\n",
    "\n",
    "How could we minimize the influence of those extreme outliers? Well, one way would be to take the log of every value.\n",
    "\n",
    "**Log scaling** does a slightly better job, but there's still a significant tail of outlier values. Let's pick yet another approach. What if we simply \"cap\" or \"clip\" the maximum value `X`.\n",
    "Clipping the feature value `X` doesn't mean that we ignore all values greater than `X`. Rather, it means that all values that were greater than `X` now become `X`. The scaled feature set is now more useful than the original data.\n",
    "\n",
    "#### Binning\n",
    "\n",
    "#### Scrubbing\n",
    "Until now, we've assumed that all the data used for training and testing was trustworthy. In real-life, many examples in data sets are unreliable due to one or more of the following:\n",
    "\n",
    "- **Omitted values.** For instance, a person forgot to enter a value for a house's age.\n",
    "- **Duplicate examples.** For example, a server mistakenly uploaded the same logs twice.\n",
    "- **Bad labels.** For instance, a person mislabeled a picture of an oak tree as a maple.\n",
    "- **Bad feature values.** For example, someone typed in an extra digit, or a thermometer was left out in the sun.\n",
    "\n",
    "Once detected, you typically \"fix\" bad examples by removing them from the data set. To detect omitted values or duplicated examples, you can write a simple program. Detecting bad feature values or labels can be far trickier.\n",
    "\n",
    "In addition to detecting bad individual examples, you must also detect bad data in the aggregate. Histograms are a great mechanism for visualizing your data in the aggregate. In addition, getting statistics like the following can help:\n",
    "\n",
    "- Maximum and minimum\n",
    "- Mean and median\n",
    "- Standard deviation\n",
    "\n",
    "Consider generating lists of the most common values for discrete features. For example, do the number of examples with `country:uk` match the number you expect. Should `language:jp` really be the most common language in your data set?\n",
    "\n",
    "#### Know your data\n",
    "\n",
    "Follow these rules:\n",
    "\n",
    "- Keep in mind what you think your data should look like.\n",
    "- Verify that the data meets these expectations (or that you can explain why it doesn’t).\n",
    "- Double-check that the training data agrees with other sources (for example, dashboards).\n",
    "\n",
    "Treat your data with all the care that you would treat any mission-critical code. Good ML relies on good data.\n",
    "\n",
    "[Additional Information](https://developers.google.com/machine-learning/rules-of-ml/#ml_phase_ii_feature_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
